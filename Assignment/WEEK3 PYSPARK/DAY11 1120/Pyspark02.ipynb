{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a32693d-dbfc-414b-ac5f-566d0508630b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# PYSPARK [Dataframe, transformation n action,views] NOV 20 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26c6c058-f8b8-4525-9117-1b3751655092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# `Transformation and Actions`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f407d64c-4881-429f-a866-1fbe4ac79a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### parallelize()\n",
    "In PySpark, parallelize() is a function used to create an RDD (Resilient Distributed Dataset) from a local Python collection.\n",
    "\n",
    "This **allows** to distribute your data across multiple nodes in a cluster, enabling parallel processing and significantly improving performance for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fb366cd-2502-48f5-a9c6-b8ac4c3e0a88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### collect()\n",
    "It is an action that retrieves all elements from an RDD or DataFrame and brings them to the driver node as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7329df2-b708-46e3-b2ca-4ae105d100fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('C', 85, 76, 87, 91), ('B', 85, 76, 87, 91), ('A', 85, 78, 96, 92), ('A', 92, 76, 89, 96)]\n\n\nParallelCollectionRDD[158] at readRDDFromInputStream at PythonRDD.scala:435\n\n\nOut[44]: [('C', 85, 76, 87, 91),\n ('B', 85, 76, 87, 91),\n ('A', 85, 78, 96, 92),\n ('A', 92, 76, 89, 96)]"
     ]
    }
   ],
   "source": [
    "\n",
    "#to create rdds and  dataframe\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import  SparkSession\n",
    "\n",
    "sc =SparkContext.getOrCreate()\n",
    "spark = SparkSession.builder.appName('pyspark first program').getOrCreate()\n",
    "\n",
    "#create the rdd\n",
    "\n",
    "rdd = sc.parallelize([('C',85,76,87,91), ('B',85,76,87,91), (\"A\", 85,78,96,92), (\"A\", 92,76,89,96)], 4)\n",
    "mydata = ['Division','English','Mathematics','Physics','Chemistry']\n",
    "marks_df = spark.createDataFrame(rdd, schema=mydata)\n",
    "\n",
    "print(rdd.collect())\n",
    "print(\"\\n\")\n",
    "print(rdd) #---Transformation which gives rdd value\n",
    "print(\"\\n\")\n",
    "rdd.collect() #----Action gives non rdd value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32fc9fec-ee05-46f0-abaf-5b4c39f28968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### first()\n",
    "It is an action that returns the first element from an RDD or DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74d0ac82-eaba-4794-a1e7-a22ed0000140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[45]: 1"
     ]
    }
   ],
   "source": [
    "first_rdd = sc.parallelize([1,2,3,4,5,5,6,7,8,9])\n",
    "first_rdd.first() #First method is action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c29860e-325e-4343-95cc-89f39ba810e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### count()\n",
    "It  is an action that returns the total number of elements in an RDD or DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c21fcccd-a573-43e2-93a3-b37662b9db9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([('C',85,76,87,91), ('B',85,76,87,91), (\"A\", 85,78,96,92), (\"A\", 92,76,89,96)], 4)\n",
    "mydata = ['Division','English','Mathematics','Physics','Chemistry']\n",
    "marks_df = spark.createDataFrame(rdd, schema=mydata)\n",
    "print(rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71ad3b2c-ba98-4638-891f-729edbeff728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\nOut[47]: 10"
     ]
    }
   ],
   "source": [
    "count_rdd = sc.parallelize([1,2,3,4,5,5,6,7,8,9])\n",
    "print(count_rdd.count())\n",
    "count_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8260014c-d973-4da7-a107-cd7b048dc13a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### take(n) \n",
    "It is an action that returns the first n elements from an RDD or DataFrame as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05e20882-131c-4cac-a90c-5bd3c5c693e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[48]: [('C', 85, 76, 87, 91), ('B', 85, 76, 87, 91)]"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([('C',85,76,87,91), ('B',85,76,87,91), (\"A\", 85,78,96,92), (\"A\", 92,76,89,96)], 4)\n",
    "mydata = ['Division','English','Mathematics','Physics','Chemistry']\n",
    "marks_df = spark.createDataFrame(rdd, schema=mydata)\n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c774fa30-6f4e-495b-95ac-e71ff800d8b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "count_rdd = sc.parallelize([1,2,3,4,5,6,7,8,9])\n",
    "print(count_rdd.take(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc17ff94-1130-4b22-acf9-e900d7ae745a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\nOut[50]: [1, 2, 3, 4, 5, 5]"
     ]
    }
   ],
   "source": [
    "count_rdd = sc.parallelize([1,2,3,4,5,5,6,7,8,9])\n",
    "print(count_rdd.take(2))\n",
    "count_rdd.take(6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "154fe333-935f-4f80-9daf-9d413984193e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Lambda function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5820f93-7a74-48ee-8d5b-9dc220b2ee0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1. map()\n",
    "It is a transformation that applies a function to each element of an RDD or DataFrame, creating a new RDD or DataFrame with the transformed elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ae565fc-d5d0-4ea2-8127-de4ef5cd131a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[193] at RDD at PythonRDD.scala:58\n[11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "map_rdd = sc.parallelize([1,2,3])\n",
    "print(map_rdd.map(lambda x:x+10))\n",
    "print(map_rdd.map(lambda x:x+10).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e98ed4e4-2332-4091-9fc5-8957dc81c130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2. reduce()\n",
    "It is an action that aggregates elements of an RDD or DataFrame into a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29f66d2b-9a71-464c-98d2-8cf1c8e486d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "reduce_rdd = sc.parallelize([1,2,3])\n",
    "print(reduce_rdd.reduce(lambda x,y:x+y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7738ffb-515c-4c49-9b8b-2874d2dff028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3. filter()\n",
    "It is a transformation that creates a new RDD or DataFrame containing only the elements that satisfy a given condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae377833-5417-439e-acb1-8e148b778597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[198] at RDD at PythonRDD.scala:58\n[2]\n"
     ]
    }
   ],
   "source": [
    "filter_rdd = sc.parallelize([1,2,3])\n",
    "print(filter_rdd.filter(lambda x:x%2==0))\n",
    "print(filter_rdd.filter(lambda x:x%2==0).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d244b7c7-4f7d-4d8a-a319-6389c4157946",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3.a startswith('')\n",
    "It is a function used to check if a string or column starts with a specific prefix. It returns a boolean value: True if the string starts with the prefix, False otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d62ca0df-cafc-4ccb-b4be-7ea0210c68c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rahul', 'Rohan']\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "# the code filters the filter_rdd_2 RDD to keep only strings starting with 'R' \n",
    "# and then prints the filtered elements to the console.\n",
    "sc = SparkContext.getOrCreate()\n",
    "filter_rdd_2 = sc.parallelize(['Rahul', 'Swati', 'Rohan', 'Shreya', 'Priya'])\n",
    "print(filter_rdd_2.filter(lambda x: x.startswith('R')).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "119af20a-c108-41fc-b229-4e194e656241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### saveAsTextFile()\n",
    "It is an action that saves the contents of an RDD or DataFrame as text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe498f0b-e6e3-4e55-9a6a-4f37bf6d5d7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "save_rdd = sc.parallelize([1,2,3,4,5,5])\n",
    "save_rdd.saveAsTextFile('/Users/vsprakash427@gmail.com/file3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c886eb73-9e2c-40a0-a528-2a4ebe767912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### union()\n",
    "It is an action that combines two or more RDDs or DataFrames with the same schema into a single RDD or DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67db1d91-d3ba-4678-add0-37e0e4817a3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "uninon_inp = sc.parallelize([2,4,5,6,7,8,9,10])\n",
    "uninon_rdd_1 = uninon_inp.filter(lambda x:x % 2 == 0)\n",
    "uninon_rdd_2 = uninon_inp.filter(lambda x:x % 3 == 0)\n",
    "print(uninon_rdd_1.union(uninon_rdd_2).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bf5cdca-e391-4750-af18-2aa7d6c37b50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### split() and flatMap()\n",
    "\n",
    "1. The split() function in PySpark is typically used to split a string column in a DataFrame into multiple columns based on a specific delimiter. The delimiter is specified as a string argument to the split() function.\n",
    "\n",
    "2. In PySpark, flatMap() is a transformation that applies a function to each element of an RDD or DataFrame, and then flattens the resulting sequence of elements into a new RDD or DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b87ba7d5-67b1-4409-93d9-4a5f9e325057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[211] at RDD at PythonRDD.scala:58\nOut[57]: PythonRDD[212] at RDD at PythonRDD.scala:58"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "flatmap_rdd = sc.parallelize([\"Hey there\", \"This is PySpark RDD Transformations\"])\n",
    "print(flatmap_rdd.flatMap(lambda x :x.split(\" \").collect()))\n",
    "flatmap_rdd.flatMap(lambda x :x.split(\" \").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97901d5d-1050-420d-bed6-5573e31abbe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[60]: ['Hey', 'there', 'This', 'is', 'PySpark', 'RDD', 'Transformations']"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "flatmap_rdd = sc.parallelize([\"Hey there\", \"This is PySpark RDD Transformations\"])\n",
    "(flatmap_rdd.flatMap(lambda x: x.split(\" \")).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9697105a-b454-4a4d-a434-ffd94ec1bc70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# `Pair RDD`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15e54a71-b0ca-4655-a046-74a94f2e0c0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### collect() - action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f2d6265-bf82-43b5-8c2e-3a42b561060c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: [('Rahul', 88), ('Swati', 92), ('Shreya', 83), ('Abhay', 93), ('Rohan', 78)]"
     ]
    }
   ],
   "source": [
    "marks = [('Rahul', 88), ('Swati', 92), ('Shreya', 83), ('Abhay', 93), ('Rohan', 78)]\n",
    "sc.parallelize(marks).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5de8c41-4fff-45ba-9c20-b15a4c6900ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### reduceByey() transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9cc3608-431b-4bd9-b4bb-9cc5837afece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Shreya', 50), ('Swati', 45), ('Rahul', 48), ('Abhay', 55), ('Rohan', 44)]\n"
     ]
    }
   ],
   "source": [
    "marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22),\n",
    "('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
    "print(marks_rdd.reduceByKey(lambda x, y: x + y).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39da2bc1-58d8-4e58-b332-146abe6cef79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### sortByKey() transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff29335-ab10-40ec-a9c3-ce16409e0c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Abhay', 29), ('Abhay', 26), ('Rahul', 25), ('Rahul', 23), ('Rohan', 22), ('Rohan', 22), ('Shreya', 22), ('Shreya', 28), ('Swati', 26), ('Swati', 19)]\n"
     ]
    }
   ],
   "source": [
    "marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22),\n",
    "('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
    "print(marks_rdd.sortByKey('ascending').collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cb52722-830f-4646-9b88-63fbe82505d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### groupByKey() transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "560f29ce-ed58-414a-9961-a1ca4f9e3c13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shreya [22, 28]\nSwati [26, 19]\nRahul [25, 23]\nAbhay [29, 26]\nRohan [22, 22]\n"
     ]
    }
   ],
   "source": [
    "marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Shreya', 22), ('Abhay', 29), ('Rohan', 22),\n",
    "('Rahul', 23), ('Swati', 19), ('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
    "dict_rdd = marks_rdd.groupByKey().collect()\n",
    "for key, value in dict_rdd:\n",
    "  print(key, list(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e07ffbba-c5bb-454d-a2f1-40452b097dc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### countByKey() action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee9a2ef-9e97-49d5-b0cb-c5723dc30721",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rahul 2\nSwati 2\nRohan 2\nShreya 1\nAbhay 1\n"
     ]
    }
   ],
   "source": [
    "marks_rdd = sc.parallelize([('Rahul', 25), ('Swati', 26), ('Rohan', 22), ('Rahul', 23), ('Swati', 19),\n",
    "('Shreya', 28), ('Abhay', 26), ('Rohan', 22)])\n",
    "dict_rdd = marks_rdd.countByKey().items()\n",
    "for key, value in dict_rdd:\n",
    "  print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f9d75d4-4fca-4bff-bafe-65736c2e52a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# `Select, Rename, Filter Data in a Pandas DF`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e639ec96-3482-4993-9f01-a71c3fe49b2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### withColumnRenamed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41a4abab-22e0-49c3-9b4f-ea493388fb6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+------+------+\n|  Name|date of birth|Gender|salary|\n+------+-------------+------+------+\n|   Ram|   1991-04-01|     M|  3000|\n|  Mike|   2000-05-19|     M|  4000|\n|Rohini|   1978-09-05|     M|  4000|\n| Maria|   1967-12-01|     F|  4000|\n| Jenis|   1980-02-17|     F|  1200|\n+------+-------------+------+------+\n\n+----------+-------------+------+------+\n|personname|date of birth|Gender|salary|\n+----------+-------------+------+------+\n|       Ram|   1991-04-01|     M|  3000|\n|      Mike|   2000-05-19|     M|  4000|\n|    Rohini|   1978-09-05|     M|  4000|\n|     Maria|   1967-12-01|     F|  4000|\n|     Jenis|   1980-02-17|     F|  1200|\n+----------+-------------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "# Create a spark session\n",
    "spark = SparkSession.builder.appName('pyspark - example join').getOrCreate()\n",
    " \n",
    "# Create data in dataframe\n",
    "data = [(('Ram'), '1991-04-01', 'M', 3000),\n",
    "        (('Mike'), '2000-05-19', 'M', 4000),\n",
    "        (('Rohini'), '1978-09-05', 'M', 4000),\n",
    "        (('Maria'), '1967-12-01', 'F', 4000),\n",
    "        (('Jenis'), '1980-02-17', 'F', 1200)]\n",
    " \n",
    "# Column names in dataframe\n",
    "columns = [\"Name\", \"DOB\", \"Gender\", \"salary\"]\n",
    " \n",
    "# Create the spark dataframe\n",
    "df = spark.createDataFrame(data=data,\n",
    "                           schema=columns)\n",
    "df.withColumnRenamed(\"DOB\",\"date of birth\").show()\n",
    "df.withColumnRenamed(\"DOB\",\"date of birth\").withColumnRenamed(\"Name\",\"personname\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7531c296-0533-42d3-b5aa-72eae8e9b10c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### selectExpr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f239546-d8f9-4638-acee-8bfb622c8d85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+------+\n|category|       DOB|  name|salary|\n+--------+----------+------+------+\n|       M|1991-04-01|   Ram|  3000|\n|       M|2000-05-19|  Mike|  4000|\n|       M|1978-09-05|Rohini|  4000|\n|       F|1967-12-01| Maria|  4000|\n|       F|1980-02-17| Jenis|  1200|\n+--------+----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries using select exp\n",
    "from pyspark.sql import SparkSession\n",
    " \n",
    "# Create a spark session\n",
    "spark = SparkSession.builder.appName('pyspark - example join').getOrCreate()\n",
    " \n",
    "# Create data in dataframe\n",
    "data = [(('Ram'), '1991-04-01', 'M', 3000),\n",
    "        (('Mike'), '2000-05-19', 'M', 4000),\n",
    "        (('Rohini'), '1978-09-05', 'M', 4000),\n",
    "        (('Maria'), '1967-12-01', 'F', 4000),\n",
    "        (('Jenis'), '1980-02-17', 'F', 1200)]\n",
    " \n",
    "# Column names in dataframe\n",
    "columns = [\"Name\", \"DOB\", \"Gender\", \"salary\"]\n",
    " \n",
    "# Create the spark dataframe\n",
    "df = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "\n",
    "data = df.selectExpr(\"Gender as category\",\"DOB\",\"Name as name\",\"salary\")\n",
    " \n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d503479-ab33-4b4c-aa7a-7fd6ed37048e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### alias() for column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbb780dd-efd1-4090-9b87-61f46fb502a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+------+\n|  Name|       DOB|Gender|Amount|\n+------+----------+------+------+\n|   Ram|1991-04-01|     M|  3000|\n|  Mike|2000-05-19|     M|  4000|\n|Rohini|1978-09-05|     M|  4000|\n| Maria|1967-12-01|     F|  4000|\n| Jenis|1980-02-17|     F|  1200|\n+------+----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    " \n",
    "# Select the 'salary' as 'Amount' using aliasing\n",
    "# Select remaining with their original name\n",
    "data = df.select(col(\"Name\"),col(\"DOB\"),\n",
    "                 col(\"Gender\"),\n",
    "                 col(\"salary\").alias('Amount'))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89bacf1c-71ef-4c32-8a95-12ae7e20939f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# `Pyspark view and temp view`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec8f054c-28e0-4586-b6d7-ee707a56b40c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### create temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9df0aedd-a036-480b-8a70-9f5c63fce5e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|    James|   Smith|    USA|   CA|\n|  Michael|    Rose|    USA|   NY|\n|   Robert|Williams|    USA|   CA|\n|    Maria|   Jones|    USA|   FL|\n+---------+--------+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Create spark session\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".appName(\"SparkByExamples.com\") \\\n",
    ".enableHiveSupport() \\\n",
    ".getOrCreate()\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "(\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "(\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "(\"Maria\",\"Jones\",\"USA\",\"FL\")\n",
    "]\n",
    "columns = [\"firstname\",\"lastname\",\"country\",\"state\"]\n",
    "# Create dataframe\n",
    "sampleDF = spark.sparkContext.parallelize(data).toDF(columns)\n",
    "sampleDF.createOrReplaceTempView(\"Person\")\n",
    "sampleDF.createOrReplaceTempView(\"mydata\")\n",
    "sampleDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7ce6ede-7974-424e-94e4-3a73cf7a588d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f423e102-bfd9-4ebe-99cc-270c20b19165",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|    James|   Smith|    USA|   CA|\n|  Michael|    Rose|    USA|   NY|\n|   Robert|Williams|    USA|   CA|\n|    Maria|   Jones|    USA|   FL|\n+---------+--------+-------+-----+\n\n+---------+--------+-------+-----+\n|firstname|lastname|country|state|\n+---------+--------+-------+-----+\n|    James|   Smith|    USA|   CA|\n|  Michael|    Rose|    USA|   NY|\n|   Robert|Williams|    USA|   CA|\n|    Maria|   Jones|    USA|   FL|\n+---------+--------+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from person\").show()\n",
    "spark.sql(\"select * from mydata\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Pyspark02",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
